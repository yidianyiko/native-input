{
  "examples": {
    "openai": {
      "provider": "openai",
      "config": {
        "api_key": "sk-your-openai-api-key-here",
        "model": "gpt-3.5-turbo",
        "base_url": null,
        "max_tokens": 1000,
        "temperature": 0.7
      }
    },
    "openai_custom": {
      "provider": "openai",
      "config": {
        "api_key": "your-api-key",
        "model": "gpt-4",
        "base_url": "https://your-custom-endpoint.com/v1",
        "max_tokens": 2000,
        "temperature": 0.5
      }
    },
    "ollama": {
      "provider": "ollama",
      "config": {
        "base_url": "http://localhost:11434",
        "model": "llama2",
        "timeout": 30
      }
    },
    "qwen": {
      "provider": "qwen",
      "config": {
        "api_key": "your-qwen-api-key",
        "model": "qwen-turbo",
        "base_url": "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
      }
    },
    "deepseek": {
      "provider": "deepseek",
      "config": {
        "api_key": "your-deepseek-api-key",
        "model": "deepseek-chat",
        "base_url": "https://api.deepseek.com/v1"
      }
    }
  },
  "instructions": {
    "setup": "Copy one of the above configurations to your config.json file under the 'llm' section",
    "api_keys": "Replace the api_key values with your actual API keys",
    "models": "You can change the model names to use different models from each provider",
    "local": "For Ollama, make sure you have Ollama running locally with the specified model"
  }
}